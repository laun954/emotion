{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f168f7-45f8-4c85-92f7-ac3f4ac9d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_9104\\2906742033.py:26: DeprecationWarning: Please import `dct` from the `scipy.fftpack` namespace; the `scipy.fftpack.realtransforms` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.fftpack.realtransforms import dct\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._inductor' from 'C:\\Users\\1\\miniconda3\\Lib\\site-packages\\torch\\_inductor\\__init__.py' has no attribute 'custom_graph_pass' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m T5EncoderModel, T5Config\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01malexnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdensenet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mefficientnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torchvision\\models\\convnext.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstochastic_depth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_presets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torchvision\\ops\\__init__.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgiou_loss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generalized_box_iou_loss\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpoolers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mps_roi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mps_roi_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_pool, PSRoIPool\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torchvision\\ops\\poolers.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mboxes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_area\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mroi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@torch\u001b[39m.jit.unused\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_onnx_merge_levels\u001b[39m(levels: Tensor, unmerged_results: List[Tensor]) -> Tensor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torchvision\\ops\\roi_align.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_compile_supported\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mannotations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pair\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\exc.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\utils.py:68\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal, TypeIs\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_functorch\\config.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Applies CSE to the graph before partitioning\u001b[39;00m\n\u001b[32m     38\u001b[39m cse = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fbcode\n\u001b[32m     43\u001b[39m enable_autograd_cache: \u001b[38;5;28mbool\u001b[39m = Config(\n\u001b[32m     44\u001b[39m     justknob=\u001b[33m\"\u001b[39m\u001b[33mpytorch/remote_cache:enable_local_autograd_cache\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     env_name_force=\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_AUTOGRAD_CACHE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     default=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremote_autograd_cache_default\u001b[39m() -> Optional[\u001b[38;5;28mbool\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_inductor\\__init__.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, IO, Optional, TYPE_CHECKING, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\_inductor\\config.py:202\u001b[39m\n\u001b[32m    195\u001b[39m b2b_gemm_pass = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# register custom graph optimization pass hook. so far, pre/post passes are\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# only applied before/after pattern_matcher in post_grad_passes.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Implement CustomGraphPass to allow Inductor to graph compiled artifacts\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# to which your custom passes have been applied:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m post_grad_custom_pre_pass: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_inductor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_graph_pass\u001b[49m.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    203\u001b[39m post_grad_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Registers a custom joint graph pass.\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch._inductor' from 'C:\\Users\\1\\miniconda3\\Lib\\site-packages\\torch\\_inductor\\__init__.py' has no attribute 'custom_graph_pass' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, Text, Button, Label, Scale, HORIZONTAL\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "is_paused = False\n",
    "cap = None\n",
    "emotion_at_time = []\n",
    "current_emotion_label = None\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import copy\n",
    "import warnings\n",
    "import csv\n",
    "import wave\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from scipy.fftpack.realtransforms import dct\n",
    "from scipy.signal import lfilter\n",
    "from scipy.signal.windows import hamming\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import T5EncoderModel, T5Config\n",
    "from features import *\n",
    "from helper import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def f(tar):\n",
    "    global emotion_at_time\n",
    "    def get_audio(path_to_wav, filename):\n",
    "        wav = wave.open(path_to_wav + filename, mode=\"r\")\n",
    "        (nchannels, sampwidth, framerate, nframes, comptype, compname) = wav.getparams()\n",
    "        content = wav.readframes(nframes)\n",
    "        samples = np.fromstring(content, dtype=np.int16)\n",
    "        return (nchannels, sampwidth, framerate, nframes, comptype, compname), samples\n",
    "    def get_transcriptions(path_to_transcriptions, filename):\n",
    "        f = open(path_to_transcriptions + filename, 'r').read()\n",
    "        f = np.array(f.split('\\n'))\n",
    "        transcription = {}\n",
    "        for i in range(len(f) - 1):\n",
    "            g = f[i]\n",
    "            i1 = g.find(': ')\n",
    "            i0 = g.find(' [')\n",
    "            ind_id = g[:i0]\n",
    "            ind_ts = g[i1+2:]\n",
    "            transcription[ind_id] = ind_ts\n",
    "        return transcription\n",
    "    batch_size = 64\n",
    "    nb_feat = 34\n",
    "    nb_class = 4\n",
    "    nb_epoch = 80\n",
    "\n",
    "    optimizer = 'Adadelta'\n",
    "    ts = []\n",
    "    \n",
    "    code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "    emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "    data_path = \"../../../data/sessions/\"\n",
    "    sessions = ['Session1','Session2','Session3','Session4','Session5']\n",
    "    framerate = 16000\n",
    "\n",
    "    def get_mocap_rot(path_to_mocap_rot, filename, start,end):\n",
    "        f = open(path_to_mocap_rot + filename, 'r').read()\n",
    "        f = np.array(f.split('\\n'))\n",
    "        mocap_rot = []\n",
    "        mocap_rot_avg = []\n",
    "        f = f[2:]\n",
    "        counter = 0\n",
    "        for data in f:\n",
    "            counter+=1\n",
    "            data2 = data.split(' ')\n",
    "            if(len(data2)<2):\n",
    "                continue\n",
    "            if(float(data2[1])>start and float(data2[1])<end):\n",
    "                mocap_rot_avg.append(np.array(data2[2:]).astype(float))\n",
    "\n",
    "        mocap_rot_avg = np.array_split(np.array(mocap_rot_avg), 200)\n",
    "        for spl in mocap_rot_avg:\n",
    "            mocap_rot.append(np.mean(spl, axis=0))\n",
    "        return np.array(mocap_rot)\n",
    "\n",
    "    def get_mocap_hand(path_to_mocap_hand, filename, start,end):\n",
    "        f = open(path_to_mocap_hand + filename, 'r').read()\n",
    "        f = np.array(f.split('\\n'))\n",
    "        mocap_hand = []\n",
    "        mocap_hand_avg = []\n",
    "        f = f[2:]\n",
    "        counter = 0\n",
    "        for data in f:\n",
    "            counter+=1\n",
    "            data2 = data.split(' ')\n",
    "            if(len(data2)<2):\n",
    "                continue\n",
    "            if(float(data2[1])>start and float(data2[1])<end):\n",
    "                mocap_hand_avg.append(np.array(data2[2:]).astype(float))\n",
    "\n",
    "        mocap_hand_avg = np.array_split(np.array(mocap_hand_avg), 200)\n",
    "        for spl in mocap_hand_avg:\n",
    "            mocap_hand.append(np.mean(spl, axis=0))\n",
    "        return np.array(mocap_hand)\n",
    "\n",
    "    def split_wav(wav, emotions):\n",
    "        (nchannels, sampwidth, framerate, nframes, comptype, compname), samples = wav\n",
    "\n",
    "        left = samples[0::nchannels]\n",
    "        right = samples[1::nchannels]\n",
    "\n",
    "        frames = []\n",
    "        for ie, e in enumerate(emotions):\n",
    "            start = e['start']\n",
    "            end = e['end']\n",
    "\n",
    "            e['right'] = right[int(start * framerate):int(end * framerate)]\n",
    "            e['left'] = left[int(start * framerate):int(end * framerate)]\n",
    "\n",
    "            frames.append({'left': e['left'], 'right': e['right']})\n",
    "        return frames\n",
    "\n",
    "    def get_emotions(path_to_emotions, filename):\n",
    "        f = open(path_to_emotions + filename, 'r').read()\n",
    "        f = np.array(f.split('\\n'))\n",
    "        idx = f == ''\n",
    "        idx_n = np.arange(len(f))[idx]\n",
    "        emotion = []\n",
    "        for i in range(len(idx_n) - 2):\n",
    "            g = f[idx_n[i]+1:idx_n[i+1]]\n",
    "            head = g[0]\n",
    "            i0 = head.find(' - ')\n",
    "            start_time = float(head[head.find('[') + 1:head.find(' - ')])\n",
    "            end_time = float(head[head.find(' - ') + 3:head.find(']')])\n",
    "            actor_id = head[head.find(filename[:-4]) + len(filename[:-4]) + 1:\n",
    "                            head.find(filename[:-4]) + len(filename[:-4]) + 5]\n",
    "            emo = head[head.find('\\t[') - 3:head.find('\\t[')]\n",
    "            vad = head[head.find('\\t[') + 1:]\n",
    "\n",
    "            v = float(vad[1:7])\n",
    "            a = float(vad[9:15])\n",
    "            d = float(vad[17:23])\n",
    "\n",
    "            j = 1\n",
    "            emos = []\n",
    "            while g[j][0] == \"C\":\n",
    "                head = g[j]\n",
    "                start_idx = head.find(\"\\t\") + 1\n",
    "                evoluator_emo = []\n",
    "                idx = head.find(\";\", start_idx)\n",
    "                while idx != -1:\n",
    "                    evoluator_emo.append(head[start_idx:idx].strip().lower()[:3])\n",
    "                    start_idx = idx + 1\n",
    "                    idx = head.find(\";\", start_idx)\n",
    "                emos.append(evoluator_emo)\n",
    "                j += 1\n",
    "\n",
    "            emotion.append({'start': start_time,\n",
    "                            'end': end_time,\n",
    "                            'id': filename[:-4] + '_' + actor_id,\n",
    "                            'v': v,\n",
    "                            'a': a,\n",
    "                            'd': d,\n",
    "                            'emotion': emo,\n",
    "                            'emo_evo': emos})\n",
    "        return emotion\n",
    "\n",
    "\n",
    "    def get_mocap_head(path_to_mocap_head, filename, start,end):\n",
    "        f = open(path_to_mocap_head + filename, 'r').read()\n",
    "        f = np.array(f.split('\\n'))\n",
    "        mocap_head = []\n",
    "        mocap_head_avg = []\n",
    "        f = f[2:]\n",
    "        counter = 0\n",
    "        for data in f:\n",
    "            counter+=1\n",
    "            data2 = data.split(' ')\n",
    "            if(len(data2)<2):\n",
    "                continue\n",
    "            if(float(data2[1])>start and float(data2[1])<end):\n",
    "                mocap_head_avg.append(np.array(data2[2:]).astype(float))\n",
    "\n",
    "        mocap_head_avg = np.array_split(np.array(mocap_head_avg), 200)\n",
    "        for spl in mocap_head_avg:\n",
    "            mocap_head.append(np.mean(spl, axis=0))\n",
    "        return np.array(mocap_head)\n",
    "\n",
    "\n",
    "\n",
    "    def get_field(data, key):\n",
    "        return np.array([e[key] for e in data])\n",
    "\n",
    "    def read_iemocap_mocap():\n",
    "        data = []\n",
    "        ids = {}\n",
    "        for session in sessions:\n",
    "            path_to_wav = data_path + session + '/dialog/wav/'\n",
    "            path_to_emotions = data_path + session + '/dialog/EmoEvaluation/'\n",
    "            path_to_transcriptions = data_path + session + '/dialog/transcriptions/'\n",
    "            path_to_mocap_hand = data_path + session + '/dialog/MOCAP_hand/'\n",
    "            path_to_mocap_rot = data_path + session + '/dialog/MOCAP_rotated/'\n",
    "            path_to_mocap_head = data_path + session + '/dialog/MOCAP_head/'\n",
    "\n",
    "            files2 = os.listdir(path_to_wav)\n",
    "\n",
    "            files = []\n",
    "            for f in files2:\n",
    "                if f.endswith(\".wav\"):\n",
    "                    if f[0] == '.':\n",
    "                        files.append(f[2:-4])\n",
    "                    else:\n",
    "                        files.append(f[:-4])\n",
    "\n",
    "\n",
    "            for f in files:\n",
    "                if f == tar:\n",
    "                    mocap_f = f\n",
    "                    if (f== 'Ses05M_script01_1b'):\n",
    "                        mocap_f = 'Ses05M_script01_1' \n",
    "\n",
    "                    wav = get_audio(path_to_wav, f + '.wav')\n",
    "                    transcriptions = get_transcriptions(path_to_transcriptions, f + '.txt')\n",
    "                    emotions = get_emotions(path_to_emotions, f + '.txt')\n",
    "\n",
    "                    sample = split_wav(wav, emotions)\n",
    "\n",
    "                    for ie, e in enumerate(emotions):\n",
    "                        '''if 'F' in e['id']:\n",
    "                            e['signal'] = sample[ie]['left']\n",
    "                        else:\n",
    "                            e['signal'] = sample[ie]['right']'''\n",
    "\n",
    "                        e['signal'] = sample[ie]['left']\n",
    "                        e.pop(\"left\", None)\n",
    "                        e.pop(\"right\", None)\n",
    "                        ts.append([e['start'], e['end']])\n",
    "                        e['transcription'] = transcriptions[e['id']]\n",
    "                        e['mocap_hand'] = get_mocap_hand(path_to_mocap_hand, mocap_f + '.txt', e['start'], e['end'])\n",
    "                        e['mocap_rot'] = get_mocap_rot(path_to_mocap_rot, mocap_f + '.txt', e['start'], e['end'])\n",
    "                        e['mocap_head'] = get_mocap_head(path_to_mocap_head, mocap_f + '.txt', e['start'], e['end'])\n",
    "                        if e['emotion'] in emotions_used:\n",
    "                            if e['id'] not in ids:\n",
    "                                data.append(e)\n",
    "                                ids[e['id']] = 1\n",
    "\n",
    "\n",
    "        sort_key = get_field(data, \"id\")\n",
    "        return np.array(data)[np.argsort(sort_key)]\n",
    "\n",
    "    data = read_iemocap_mocap()\n",
    "\n",
    "    import pickle\n",
    "    with open( './result.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def pad_sequence_into_array(Xs, maxlen=None, truncating='post', padding='post', value=0.):\n",
    "\n",
    "        Nsamples = len(Xs)\n",
    "        if maxlen is None:\n",
    "            lengths = [s.shape[0] for s in Xs]    # 'sequences' must be list, 's' must be numpy array, len(s) return the first dimension of s\n",
    "            maxlen = np.max(lengths)\n",
    "\n",
    "        Xout = np.ones(shape=[Nsamples, maxlen] + list(Xs[0].shape[1:]), dtype=Xs[0].dtype) * np.asarray(value, dtype=Xs[0].dtype)\n",
    "        Mask = np.zeros(shape=[Nsamples, maxlen], dtype=Xout.dtype)\n",
    "        for i in range(Nsamples):\n",
    "            x = Xs[i]\n",
    "            if truncating == 'pre':\n",
    "                trunc = x[-maxlen:]\n",
    "            elif truncating == 'post':\n",
    "                trunc = x[:maxlen]\n",
    "            else:\n",
    "                raise ValueError(\"Truncating type '%s' not understood\" % truncating)\n",
    "            if padding == 'post':\n",
    "                Xout[i, :len(trunc)] = trunc\n",
    "                Mask[i, :len(trunc)] = 1\n",
    "            elif padding == 'pre':\n",
    "                Xout[i, -len(trunc):] = trunc\n",
    "                Mask[i, -len(trunc):] = 1\n",
    "            else:\n",
    "                raise ValueError(\"Padding type '%s' not understood\" % padding)\n",
    "        return Xout, Mask\n",
    "\n",
    "\n",
    "    def convert_gt_from_array_to_list(gt_batch, gt_batch_mask=None):\n",
    "\n",
    "        B, L = gt_batch.shape\n",
    "        gt_batch = gt_batch.astype('int')\n",
    "        gts = []\n",
    "        for i in range(B):\n",
    "            if gt_batch_mask is None:\n",
    "                l = L\n",
    "            else:\n",
    "                l = int(gt_batch_mask[i, :].sum())\n",
    "            gts.append(gt_batch[i, :l].tolist())\n",
    "        return gts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    eps = 0.00000001\n",
    "\n",
    "\n",
    "    \"\"\" Time-domain audio features \"\"\"\n",
    "\n",
    "\n",
    "    def stZCR(frame):\n",
    "        \"\"\"Computes zero crossing rate of frame\"\"\"\n",
    "        count = len(frame)\n",
    "        countZ = numpy.sum(numpy.abs(numpy.diff(numpy.sign(frame)))) / 2\n",
    "        return (numpy.float64(countZ) / numpy.float64(count-1.0))\n",
    "\n",
    "\n",
    "    def stEnergy(frame):\n",
    "        \"\"\"Computes signal energy of frame\"\"\"\n",
    "        return numpy.sum(frame ** 2) / numpy.float64(len(frame))\n",
    "\n",
    "\n",
    "    def stEnergyEntropy(frame, numOfShortBlocks=10):\n",
    "        \"\"\"Computes entropy of energy\"\"\"\n",
    "        Eol = numpy.sum(frame ** 2)    # total frame energy\n",
    "        L = len(frame)\n",
    "        subWinLength = int(numpy.floor(L / numOfShortBlocks))\n",
    "        if L != subWinLength * numOfShortBlocks:\n",
    "                frame = frame[0:subWinLength * numOfShortBlocks]\n",
    "        # subWindows is of size [numOfShortBlocks x L]\n",
    "        subWindows = frame.reshape(subWinLength, numOfShortBlocks, order='F').copy()\n",
    "\n",
    "        # Compute normalized sub-frame energies:\n",
    "        s = numpy.sum(subWindows ** 2, axis=0) / (Eol + eps)\n",
    "\n",
    "        # Compute entropy of the normalized sub-frame energies:\n",
    "        Entropy = -numpy.sum(s * numpy.log2(s + eps))\n",
    "        return Entropy\n",
    "\n",
    "\n",
    "    \"\"\" Frequency-domain audio features \"\"\"\n",
    "\n",
    "\n",
    "    def stSpectralCentroidAndSpread(X, fs):\n",
    "        \"\"\"Computes spectral centroid of frame (given abs(FFT))\"\"\"\n",
    "        ind = (numpy.arange(1, len(X) + 1)) * (fs/(2.0 * len(X)))\n",
    "\n",
    "        Xt = X.copy()\n",
    "        Xt = Xt / Xt.max()\n",
    "        NUM = numpy.sum(ind * Xt)\n",
    "        DEN = numpy.sum(Xt) + eps\n",
    "\n",
    "        # Centroid:\n",
    "        C = (NUM / DEN)\n",
    "\n",
    "        # Spread:\n",
    "        S = numpy.sqrt(numpy.sum(((ind - C) ** 2) * Xt) / DEN)\n",
    "\n",
    "        # Normalize:\n",
    "        C = C / (fs / 2.0)\n",
    "        S = S / (fs / 2.0)\n",
    "\n",
    "        return (C, S)\n",
    "\n",
    "\n",
    "    def stSpectralEntropy(X, numOfShortBlocks=10):\n",
    "        \"\"\"Computes the spectral entropy\"\"\"\n",
    "        L = len(X)                         # number of frame samples\n",
    "        Eol = numpy.sum(X ** 2)            # total spectral energy\n",
    "\n",
    "        subWinLength = int(numpy.floor(L / numOfShortBlocks))   # length of sub-frame\n",
    "        if L != subWinLength * numOfShortBlocks:\n",
    "            X = X[0:subWinLength * numOfShortBlocks]\n",
    "\n",
    "        subWindows = X.reshape(subWinLength, numOfShortBlocks, order='F').copy()  # define sub-frames (using matrix reshape)\n",
    "        s = numpy.sum(subWindows ** 2, axis=0) / (Eol + eps)                      # compute spectral sub-energies\n",
    "        En = -numpy.sum(s*numpy.log2(s + eps))                                    # compute spectral entropy\n",
    "\n",
    "        return En\n",
    "\n",
    "\n",
    "    def stSpectralFlux(X, Xprev):\n",
    "        \"\"\"\n",
    "        Computes the spectral flux feature of the current frame\n",
    "        ARGUMENTS:\n",
    "            X:        the abs(fft) of the current frame\n",
    "            Xpre:        the abs(fft) of the previous frame\n",
    "        \"\"\"\n",
    "        # compute the spectral flux as the sum of square distances:\n",
    "        sumX = numpy.sum(X + eps)\n",
    "        sumPrevX = numpy.sum(Xprev + eps)\n",
    "        F = numpy.sum((X / sumX - Xprev/sumPrevX) ** 2)\n",
    "\n",
    "        return F\n",
    "\n",
    "\n",
    "    def stSpectralRollOff(X, c, fs):\n",
    "        \"\"\"Computes spectral roll-off\"\"\"\n",
    "        totalEnergy = numpy.sum(X ** 2)\n",
    "        fftLength = len(X)\n",
    "        Thres = c*totalEnergy\n",
    "        # Ffind the spectral rolloff as the frequency position where the respective spectral energy is equal to c*totalEnergy\n",
    "        CumSum = numpy.cumsum(X ** 2) + eps\n",
    "        [a, ] = numpy.nonzero(CumSum > Thres)\n",
    "        if len(a) > 0:\n",
    "            mC = numpy.float64(a[0]) / (float(fftLength))\n",
    "        else:\n",
    "            mC = 0.0\n",
    "        return (mC)\n",
    "\n",
    "\n",
    "    def stHarmonic(frame, fs):\n",
    "        \"\"\"\n",
    "        Computes harmonic ratio and pitch\n",
    "        \"\"\"\n",
    "        M = numpy.round(0.016 * fs) - 1\n",
    "        R = numpy.correlate(frame, frame, mode='full')\n",
    "\n",
    "        g = R[len(frame)-1]\n",
    "        R = R[len(frame):-1]\n",
    "\n",
    "        # estimate m0 (as the first zero crossing of R)\n",
    "        [a, ] = numpy.nonzero(numpy.diff(numpy.sign(R)))\n",
    "\n",
    "        if len(a) == 0:\n",
    "            m0 = len(R)-1\n",
    "        else:\n",
    "            m0 = a[0]\n",
    "        if M > len(R):\n",
    "            M = len(R) - 1\n",
    "\n",
    "        Gamma = numpy.zeros((M), dtype=numpy.float64)\n",
    "        CSum = numpy.cumsum(frame ** 2)\n",
    "        Gamma[m0:M] = R[m0:M] / (numpy.sqrt((g * CSum[M:m0:-1])) + eps)\n",
    "\n",
    "        ZCR = stZCR(Gamma)\n",
    "\n",
    "        if ZCR > 0.15:\n",
    "            HR = 0.0\n",
    "            f0 = 0.0\n",
    "        else:\n",
    "            if len(Gamma) == 0:\n",
    "                HR = 1.0\n",
    "                blag = 0.0\n",
    "                Gamma = numpy.zeros((M), dtype=numpy.float64)\n",
    "            else:\n",
    "                HR = numpy.max(Gamma)\n",
    "                blag = numpy.argmax(Gamma)\n",
    "\n",
    "            # Get fundamental frequency:\n",
    "            f0 = fs / (blag + eps)\n",
    "            if f0 > 5000:\n",
    "                f0 = 0.0\n",
    "            if HR < 0.1:\n",
    "                f0 = 0.0\n",
    "\n",
    "        return (HR, f0)\n",
    "\n",
    "\n",
    "    def mfccInitFilterBanks(fs, nfft):\n",
    "        \"\"\"\n",
    "        Computes the triangular filterbank for MFCC computation (used in the stFeatureExtraction function before the stMFCC function call)\n",
    "        This function is taken from the scikits.talkbox library (MIT Licence):\n",
    "        https://pypi.python.org/pypi/scikits.talkbox\n",
    "        \"\"\"\n",
    "\n",
    "        # filter bank params:\n",
    "        lowfreq = 133.33\n",
    "        linsc = 200/3.\n",
    "        logsc = 1.0711703\n",
    "        numLinFiltTotal = 13\n",
    "        numLogFilt = 27\n",
    "\n",
    "        if fs < 8000:\n",
    "            nlogfil = 5\n",
    "\n",
    "        # Total number of filters\n",
    "        nFiltTotal = numLinFiltTotal + numLogFilt\n",
    "        #print (str(nFiltTotal))\n",
    "        #print (str(nfft))\n",
    "\n",
    "        # Compute frequency points of the triangle:\n",
    "        freqs = numpy.zeros(nFiltTotal+2)\n",
    "        freqs[:numLinFiltTotal] = lowfreq + numpy.arange(numLinFiltTotal) * linsc\n",
    "        freqs[numLinFiltTotal:] = freqs[numLinFiltTotal-1] * logsc ** numpy.arange(1, numLogFilt + 3)\n",
    "        heights = 2./(freqs[2:] - freqs[0:-2])\n",
    "\n",
    "        # Compute filterbank coeff (in fft domain, in bins)\n",
    "\n",
    "\n",
    "        fbank = numpy.zeros((int(nFiltTotal), int(nfft)))\n",
    "        nfreqs = numpy.arange(nfft) / (1. * nfft) * fs\n",
    "\n",
    "        for i in range(nFiltTotal):\n",
    "            lowTrFreq = freqs[i]\n",
    "            cenTrFreq = freqs[i+1]\n",
    "            highTrFreq = freqs[i+2]\n",
    "\n",
    "            lid = numpy.arange(numpy.floor(lowTrFreq * nfft / fs) + 1, numpy.floor(cenTrFreq * nfft / fs) + 1, dtype=int)\n",
    "            lslope = heights[i] / (cenTrFreq - lowTrFreq)\n",
    "            rid = numpy.arange(numpy.floor(cenTrFreq * nfft / fs) + 1, numpy.floor(highTrFreq * nfft / fs) + 1, dtype=int)\n",
    "            rslope = heights[i] / (highTrFreq - cenTrFreq)\n",
    "            fbank[i][lid] = lslope * (nfreqs[lid] - lowTrFreq)\n",
    "            fbank[i][rid] = rslope * (highTrFreq - nfreqs[rid])\n",
    "\n",
    "        return fbank, freqs\n",
    "\n",
    "\n",
    "    def stMFCC(X, fbank, nceps):\n",
    "        \"\"\"\n",
    "        Computes the MFCCs of a frame, given the fft mag\n",
    "        ARGUMENTS:\n",
    "            X:        fft magnitude abs(FFT)\n",
    "            fbank:    filter bank (see mfccInitFilterBanks)\n",
    "        RETURN\n",
    "            ceps:     MFCCs (13 element vector)\n",
    "        Note:    MFCC calculation is, in general, taken from the scikits.talkbox library (MIT Licence),\n",
    "        #    with a small number of modifications to make it more compact and suitable for the pyAudioAnalysis Lib\n",
    "        \"\"\"\n",
    "\n",
    "        mspec = numpy.log10(numpy.dot(X, fbank.T)+eps)\n",
    "        ceps = dct(mspec, type=2, norm='ortho', axis=-1)[:nceps]\n",
    "        return ceps\n",
    "\n",
    "\n",
    "    def stChromaFeaturesInit(nfft, fs):\n",
    "        \"\"\"\n",
    "        This function initializes the chroma matrices used in the calculation of the chroma features\n",
    "        \"\"\"\n",
    "        freqs = numpy.array([((f + 1) * fs) / (2 * int(nfft)) for f in range(int(nfft))])\n",
    "        Cp = 27.50\n",
    "\n",
    "        nChroma = numpy.round(12.0 * numpy.log2(freqs / Cp)).astype(int)\n",
    "\n",
    "        nFreqsPerChroma = numpy.zeros((nChroma.shape[0], ))\n",
    "\n",
    "        uChroma = numpy.unique(nChroma)\n",
    "        for u in uChroma:\n",
    "            idx = numpy.nonzero(nChroma == u)\n",
    "            nFreqsPerChroma[idx] = idx[0].shape\n",
    "        return nChroma, nFreqsPerChroma\n",
    "\n",
    "\n",
    "    def stChromaFeatures(X, fs, nChroma, nFreqsPerChroma):\n",
    "        #TODO: 1 complexity\n",
    "        #TODO: 2 bug with large windows\n",
    "\n",
    "        chromaNames = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#']\n",
    "        spec = X**2\n",
    "        C = numpy.zeros((nChroma.shape[0],))\n",
    "        C[nChroma] = spec\n",
    "        C /= nFreqsPerChroma[nChroma]\n",
    "        finalC = numpy.zeros((12, 1))\n",
    "        newD = int(numpy.ceil(C.shape[0] // 12.0) * 12)\n",
    "        C2 = numpy.zeros((newD, ))\n",
    "        #C2[0:C.shape[0]] = C\n",
    "        C2 = C2.reshape(int(numpy.ceil(C2.shape[0]//12)), 12)\n",
    "        #for i in range(12):\n",
    "        #    finalC[i] = numpy.sum(C[i:C.shape[0]:12])\n",
    "        finalC = numpy.matrix(numpy.sum(C2, axis=0)).T\n",
    "        finalC /= spec.sum()\n",
    "\n",
    "    #    ax = plt.gca()\n",
    "    #    plt.hold(False)\n",
    "    #    plt.plot(finalC)\n",
    "    #    ax.set_xticks(range(len(chromaNames)))\n",
    "    #    ax.set_xticklabels(chromaNames)\n",
    "    #    xaxis = numpy.arange(0, 0.02, 0.01);\n",
    "    #    ax.set_yticks(range(len(xaxis)))\n",
    "    #    ax.set_yticklabels(xaxis)\n",
    "    #    plt.show(block=False)\n",
    "    #    plt.draw()\n",
    "\n",
    "        return chromaNames, finalC\n",
    "\n",
    "\n",
    "    def stChromagram(signal, Fs, Win, Step, PLOT=False):\n",
    "        \"\"\"\n",
    "        Short-term FFT mag for spectogram estimation:\n",
    "        Returns:\n",
    "            a numpy array (nFFT x numOfShortTermWindows)\n",
    "        ARGUMENTS:\n",
    "            signal:      the input signal samples\n",
    "            Fs:          the sampling freq (in Hz)\n",
    "            Win:         the short-term window size (in samples)\n",
    "            Step:        the short-term window step (in samples)\n",
    "            PLOT:        flag, 1 if results are to be ploted\n",
    "        RETURNS:\n",
    "        \"\"\"\n",
    "        Win = int(Win)\n",
    "        Step = int(Step)\n",
    "        signal = numpy.double(signal)\n",
    "        signal = signal / (2.0 ** 15)\n",
    "        DC = signal.mean()\n",
    "        MAX = (numpy.abs(signal)).max()\n",
    "        signal = (signal - DC) / (MAX - DC)\n",
    "\n",
    "        N = len(signal)        # total number of signals\n",
    "        curPos = 0\n",
    "        countFrames = 0\n",
    "        nfft = int(Win / 2)\n",
    "        nChroma, nFreqsPerChroma = stChromaFeaturesInit(nfft, Fs)\n",
    "        chromaGram = numpy.array([], dtype=numpy.float64)\n",
    "\n",
    "        while (curPos + Win - 1 < N):\n",
    "            countFrames += 1\n",
    "            x = signal[curPos:curPos + Win]\n",
    "            curPos = curPos + Step\n",
    "            X = abs(fft(x))\n",
    "            X = X[0:nfft]\n",
    "            X = X / len(X)\n",
    "            chromaNames, C = stChromaFeatures(X, Fs, nChroma, nFreqsPerChroma)\n",
    "            C = C[:, 0]\n",
    "            if countFrames == 1:\n",
    "                chromaGram = C.T\n",
    "            else:\n",
    "                chromaGram = numpy.vstack((chromaGram, C.T))\n",
    "        FreqAxis = chromaNames\n",
    "        TimeAxis = [(t * Step) / Fs for t in range(chromaGram.shape[0])]\n",
    "\n",
    "        if (PLOT):\n",
    "            fig, ax = plt.subplots()\n",
    "            chromaGramToPlot = chromaGram.transpose()[::-1, :]\n",
    "            Ratio = chromaGramToPlot.shape[1] / (3*chromaGramToPlot.shape[0])\n",
    "            chromaGramToPlot = numpy.repeat(chromaGramToPlot, Ratio, axis=0)\n",
    "            imgplot = plt.imshow(chromaGramToPlot)\n",
    "            Fstep = int(nfft / 5.0)\n",
    "    #        FreqTicks = range(0, int(nfft) + Fstep, Fstep)\n",
    "    #        FreqTicksLabels = [str(Fs/2-int((f*Fs) / (2*nfft))) for f in FreqTicks]\n",
    "            ax.set_yticks(range(Ratio / 2, len(FreqAxis) * Ratio, Ratio))\n",
    "            ax.set_yticklabels(FreqAxis[::-1])\n",
    "            TStep = countFrames / 3\n",
    "            TimeTicks = range(0, countFrames, TStep)\n",
    "            TimeTicksLabels = ['%.2f' % (float(t * Step) / Fs) for t in TimeTicks]\n",
    "            ax.set_xticks(TimeTicks)\n",
    "            ax.set_xticklabels(TimeTicksLabels)\n",
    "            ax.set_xlabel('time (secs)')\n",
    "            imgplot.set_cmap('jet')\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "        return (chromaGram, TimeAxis, FreqAxis)\n",
    "\n",
    "\n",
    "    def phormants(x, Fs):\n",
    "        N = len(x)\n",
    "        w = numpy.hamming(N)\n",
    "\n",
    "        # Apply window and high pass filter.\n",
    "        x1 = x * w   \n",
    "        x1 = lfilter([1], [1., 0.63], x1)\n",
    "\n",
    "        # Get LPC.    \n",
    "        ncoeff = 2 + Fs / 1000\n",
    "        A, e, k = lpc(x1, ncoeff)    \n",
    "        #A, e, k = lpc(x1, 8)\n",
    "\n",
    "        # Get roots.\n",
    "        rts = numpy.roots(A)\n",
    "        rts = [r for r in rts if numpy.imag(r) >= 0]\n",
    "\n",
    "        # Get angles.\n",
    "        angz = numpy.arctan2(numpy.imag(rts), numpy.real(rts))\n",
    "\n",
    "        # Get frequencies.    \n",
    "        frqs = sorted(angz * (Fs / (2 * math.pi)))\n",
    "\n",
    "        return frqs\n",
    "    def beatExtraction(stFeatures, winSize, PLOT=False):\n",
    "        \"\"\"\n",
    "        This function extracts an estimate of the beat rate for a musical signal.\n",
    "        ARGUMENTS:\n",
    "         - stFeatures:     a numpy array (numOfFeatures x numOfShortTermWindows)\n",
    "         - winSize:        window size in seconds\n",
    "        RETURNS:\n",
    "         - BPM:            estimates of beats per minute\n",
    "         - Ratio:          a confidence measure\n",
    "        \"\"\"\n",
    "\n",
    "        # Features that are related to the beat tracking task:\n",
    "        toWatch = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "        maxBeatTime = int(round(2.0 / winSize))\n",
    "        HistAll = numpy.zeros((maxBeatTime,))\n",
    "        for ii, i in enumerate(toWatch):                                        # for each feature\n",
    "            DifThres = 2.0 * (numpy.abs(stFeatures[i, 0:-1] - stFeatures[i, 1::])).mean()    # dif threshold (3 x Mean of Difs)\n",
    "            [pos1, _] = utilities.peakdet(stFeatures[i, :], DifThres)           # detect local maxima\n",
    "            posDifs = []                                                        # compute histograms of local maxima changes\n",
    "            for j in range(len(pos1)-1):\n",
    "                posDifs.append(pos1[j+1]-pos1[j])\n",
    "            [HistTimes, HistEdges] = numpy.histogram(posDifs, numpy.arange(0.5, maxBeatTime + 1.5))\n",
    "            HistCenters = (HistEdges[0:-1] + HistEdges[1::]) / 2.0\n",
    "            HistTimes = HistTimes.astype(float) / stFeatures.shape[1]\n",
    "            HistAll += HistTimes\n",
    "            if PLOT:\n",
    "                plt.subplot(9, 2, ii + 1)\n",
    "                plt.plot(stFeatures[i, :], 'k')\n",
    "                for k in pos1:\n",
    "                    plt.plot(k, stFeatures[i, k], 'k*')\n",
    "                f1 = plt.gca()\n",
    "                f1.axes.get_xaxis().set_ticks([])\n",
    "                f1.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "        if PLOT:\n",
    "            plt.show(block=False)\n",
    "            plt.figure()\n",
    "\n",
    "        # Get beat as the argmax of the agregated histogram:\n",
    "        I = numpy.argmax(HistAll)\n",
    "        BPMs = 60 / (HistCenters * winSize)\n",
    "        BPM = BPMs[I]\n",
    "        # ... and the beat ratio:\n",
    "        Ratio = HistAll[I] / HistAll.sum()\n",
    "\n",
    "        if PLOT:\n",
    "            # filter out >500 beats from plotting:\n",
    "            HistAll = HistAll[BPMs < 500]\n",
    "            BPMs = BPMs[BPMs < 500]\n",
    "\n",
    "            plt.plot(BPMs, HistAll, 'k')\n",
    "            plt.xlabel('Beats per minute')\n",
    "            plt.ylabel('Freq Count')\n",
    "            plt.show(block=True)\n",
    "\n",
    "        return BPM, Ratio\n",
    "\n",
    "\n",
    "    def stSpectogram(signal, Fs, Win, Step, PLOT=False):\n",
    "        \"\"\"\n",
    "        Short-term FFT mag for spectogram estimation:\n",
    "        Returns:\n",
    "            a numpy array (nFFT x numOfShortTermWindows)\n",
    "        ARGUMENTS:\n",
    "            signal:      the input signal samples\n",
    "            Fs:          the sampling freq (in Hz)\n",
    "            Win:         the short-term window size (in samples)\n",
    "            Step:        the short-term window step (in samples)\n",
    "            PLOT:        flag, 1 if results are to be ploted\n",
    "        RETURNS:\n",
    "        \"\"\"\n",
    "        Win = int(Win)\n",
    "        Step = int(Step)\n",
    "        signal = numpy.double(signal)\n",
    "        signal = signal / (2.0 ** 15)\n",
    "        DC = signal.mean()\n",
    "        MAX = (numpy.abs(signal)).max()\n",
    "        signal = (signal - DC) / (MAX - DC)\n",
    "\n",
    "        N = len(signal)        # total number of signals\n",
    "        curPos = 0\n",
    "        countFrames = 0\n",
    "        nfft = int(Win / 2)\n",
    "        specgram = numpy.array([], dtype=numpy.float64)\n",
    "\n",
    "        while (curPos + Win - 1 < N):\n",
    "            countFrames += 1\n",
    "            x = signal[curPos:curPos+Win]\n",
    "            curPos = curPos + Step\n",
    "            X = abs(fft(x))\n",
    "            X = X[0:nfft]\n",
    "            X = X / len(X)\n",
    "\n",
    "            if countFrames == 1:\n",
    "                specgram = X ** 2\n",
    "            else:\n",
    "                specgram = numpy.vstack((specgram, X))\n",
    "\n",
    "        FreqAxis = [((f + 1) * Fs) / (2 * nfft) for f in range(specgram.shape[1])]\n",
    "        TimeAxis = [(t * Step) / Fs for t in range(specgram.shape[0])]\n",
    "\n",
    "        if (PLOT):\n",
    "            fig, ax = plt.subplots()\n",
    "            imgplot = plt.imshow(specgram.transpose()[::-1, :])\n",
    "            Fstep = int(nfft / 5.0)\n",
    "            FreqTicks = range(0, int(nfft) + Fstep, Fstep)\n",
    "            FreqTicksLabels = [str(Fs / 2 - int((f * Fs) / (2 * nfft))) for f in FreqTicks]\n",
    "            ax.set_yticks(FreqTicks)\n",
    "            ax.set_yticklabels(FreqTicksLabels)\n",
    "            TStep = countFrames/3\n",
    "            TimeTicks = range(0, countFrames, TStep)\n",
    "            TimeTicksLabels = ['%.2f' % (float(t * Step) / Fs) for t in TimeTicks]\n",
    "            ax.set_xticks(TimeTicks)\n",
    "            ax.set_xticklabels(TimeTicksLabels)\n",
    "            ax.set_xlabel('time (secs)')\n",
    "            ax.set_ylabel('freq (Hz)')\n",
    "            imgplot.set_cmap('jet')\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "        return (specgram, TimeAxis, FreqAxis)\n",
    "\n",
    "\n",
    "    \"\"\" Windowing and feature extraction \"\"\"\n",
    "\n",
    "\n",
    "    def stFeatureExtraction(signal, Fs, Win, Step):\n",
    "        \"\"\"\n",
    "        This function implements the shor-term windowing process. For each short-term window a set of features is extracted.\n",
    "        This results to a sequence of feature vectors, stored in a numpy matrix.\n",
    "        ARGUMENTS\n",
    "            signal:       the input signal samples\n",
    "            Fs:           the sampling freq (in Hz)\n",
    "            Win:          the short-term window size (in samples)\n",
    "            Step:         the short-term window step (in samples)\n",
    "        RETURNS\n",
    "            stFeatures:   a numpy array (numOfFeatures x numOfShortTermWindows)\n",
    "        \"\"\"\n",
    "\n",
    "        Win = int(Win)\n",
    "        Step = int(Step)\n",
    "\n",
    "        # Signal normalization\n",
    "        signal = numpy.double(signal)\n",
    "\n",
    "        signal = signal / (2.0 ** 15)\n",
    "        DC = signal.mean()\n",
    "        MAX = (numpy.abs(signal)).max()\n",
    "        signal = (signal - DC) / MAX\n",
    "\n",
    "        N = len(signal)                                # total number of samples\n",
    "        curPos = 0\n",
    "        countFrames = 0\n",
    "        nFFT = Win / 2\n",
    "\n",
    "        [fbank, freqs] = mfccInitFilterBanks(Fs, nFFT)                # compute the triangular filter banks used in the mfcc calculation\n",
    "        nChroma, nFreqsPerChroma = stChromaFeaturesInit(nFFT, Fs)\n",
    "\n",
    "\n",
    "        numOfPitches = 5\n",
    "        numOfPeaks = 10\n",
    "        numOfTimeSpectralFeatures = 8\n",
    "        numOfHarmonicFeatures = 0\n",
    "        nceps = 13\n",
    "        numOfChromaFeatures = 13\n",
    "\n",
    "        totalNumOfFeatures = numOfTimeSpectralFeatures + nceps + numOfHarmonicFeatures + numOfChromaFeatures\n",
    "    #    totalNumOfFeatures = numOfTimeSpectralFeatures + nceps + numOfHarmonicFeatures\n",
    "        stFeatures = numpy.array([], dtype=numpy.float64)\n",
    "\n",
    "        while (curPos + Win - 1 < N):                        # for each short-term window until the end of signal\n",
    "            countFrames += 1\n",
    "            x = signal[curPos:curPos+Win]                    # get current window\n",
    "            curPos = curPos + Step                           # update window position\n",
    "            X = abs(fft(x))                                  # get fft magnitude\n",
    "            X = X[0:int(nFFT)]                                    # normalize fft\n",
    "            X = X / len(X)\n",
    "            if countFrames == 1:\n",
    "                Xprev = X.copy()                             # keep previous fft mag (used in spectral flux)\n",
    "            curFV = numpy.zeros((totalNumOfFeatures, 1))\n",
    "            curFV[0] = stZCR(x)                              # zero crossing rate\n",
    "            curFV[1] = stEnergy(x)                           # short-term energy\n",
    "            curFV[2] = stEnergyEntropy(x)                    # short-term entropy of energy\n",
    "            [curFV[3], curFV[4]] = stSpectralCentroidAndSpread(X, Fs)    # spectral centroid and spread\n",
    "            curFV[5] = stSpectralEntropy(X)                  # spectral entropy\n",
    "            curFV[6] = stSpectralFlux(X, Xprev)              # spectral flux\n",
    "            curFV[7] = stSpectralRollOff(X, 0.90, Fs)        # spectral rolloff\n",
    "            curFV[numOfTimeSpectralFeatures:numOfTimeSpectralFeatures+nceps, 0] = stMFCC(X, fbank, nceps).copy()    # MFCCs\n",
    "\n",
    "            chromaNames, chromaF = stChromaFeatures(X, Fs, nChroma, nFreqsPerChroma)\n",
    "            curFV[numOfTimeSpectralFeatures + nceps: numOfTimeSpectralFeatures + nceps + numOfChromaFeatures - 1] = chromaF\n",
    "            numOfCFFeatures = numOfTimeSpectralFeatures + nceps + numOfChromaFeatures\n",
    "\n",
    "            curFV[numOfCFFeatures-1] = chromaF.std()\n",
    "\n",
    "            if countFrames == 1:\n",
    "                stFeatures = curFV                                        # initialize feature matrix (if first frame)\n",
    "            else:\n",
    "                stFeatures = numpy.concatenate((stFeatures, curFV), 1)    # update feature matrix\n",
    "            Xprev = X.copy()\n",
    "\n",
    "        return numpy.array(stFeatures)\n",
    "\n",
    "\n",
    "    def mtFeatureExtraction(signal, Fs, mtWin, mtStep, stWin, stStep):\n",
    "        \"\"\"\n",
    "        Mid-term feature extraction\n",
    "        \"\"\"\n",
    "\n",
    "        mtWinRatio = int(round(mtWin / stStep))\n",
    "        mtStepRatio = int(round(mtStep / stStep))\n",
    "\n",
    "        mtFeatures = []\n",
    "\n",
    "        stFeatures = stFeatureExtraction(signal, Fs, stWin, stStep)\n",
    "        numOfFeatures = len(stFeatures)\n",
    "        numOfStatistics = 2\n",
    "\n",
    "        mtFeatures = []\n",
    "        #for i in range(numOfStatistics * numOfFeatures + 1):\n",
    "        for i in range(numOfStatistics * numOfFeatures):\n",
    "            mtFeatures.append([])\n",
    "\n",
    "        for i in range(numOfFeatures):        # for each of the short-term features:\n",
    "            curPos = 0\n",
    "            N = len(stFeatures[i])\n",
    "            while (curPos < N):\n",
    "                N1 = curPos\n",
    "                N2 = curPos + mtWinRatio\n",
    "                if N2 > N:\n",
    "                    N2 = N\n",
    "                curStFeatures = stFeatures[i][N1:N2]\n",
    "\n",
    "                mtFeatures[i].append(numpy.mean(curStFeatures))\n",
    "                mtFeatures[i+numOfFeatures].append(numpy.std(curStFeatures))\n",
    "                #mtFeatures[i+2*numOfFeatures].append(numpy.std(curStFeatures) / (numpy.mean(curStFeatures)+0.00000010))\n",
    "                curPos += mtStepRatio\n",
    "\n",
    "        return numpy.array(mtFeatures), stFeatures\n",
    "\n",
    "\n",
    "    # TODO\n",
    "    def stFeatureSpeed(signal, Fs, Win, Step):\n",
    "\n",
    "        signal = numpy.double(signal)\n",
    "        signal = signal / (2.0 ** 15)\n",
    "        DC = signal.mean()\n",
    "        MAX = (numpy.abs(signal)).max()\n",
    "        signal = (signal - DC) / MAX\n",
    "        # print (numpy.abs(signal)).max()\n",
    "\n",
    "        N = len(signal)        # total number of signals\n",
    "        curPos = 0\n",
    "        countFrames = 0\n",
    "\n",
    "        lowfreq = 133.33\n",
    "        linsc = 200/3.\n",
    "        logsc = 1.0711703\n",
    "        nlinfil = 13\n",
    "        nlogfil = 27\n",
    "        nceps = 13\n",
    "        nfil = nlinfil + nlogfil\n",
    "        nfft = Win / 2\n",
    "        if Fs < 8000:\n",
    "            nlogfil = 5\n",
    "            nfil = nlinfil + nlogfil\n",
    "            nfft = Win / 2\n",
    "\n",
    "        # compute filter banks for mfcc:\n",
    "        [fbank, freqs] = mfccInitFilterBanks(Fs, nfft, lowfreq, linsc, logsc, nlinfil, nlogfil)\n",
    "\n",
    "        numOfTimeSpectralFeatures = 8\n",
    "        numOfHarmonicFeatures = 1\n",
    "        totalNumOfFeatures = numOfTimeSpectralFeatures + nceps + numOfHarmonicFeatures\n",
    "        #stFeatures = numpy.array([], dtype=numpy.float64)\n",
    "        stFeatures = []\n",
    "\n",
    "        while (curPos + Win - 1 < N):\n",
    "            countFrames += 1\n",
    "            x = signal[curPos:curPos + Win]\n",
    "            curPos = curPos + Step\n",
    "            X = abs(fft(x))\n",
    "            X = X[0:nfft]\n",
    "            X = X / len(X)\n",
    "            Ex = 0.0\n",
    "            El = 0.0\n",
    "            X[0:4] = 0\n",
    "    #        M = numpy.round(0.016 * fs) - 1\n",
    "    #        R = numpy.correlate(frame, frame, mode='full')\n",
    "            stFeatures.append(stHarmonic(x, Fs))\n",
    "    #        for i in range(len(X)):\n",
    "                #if (i < (len(X) / 8)) and (i > (len(X)/40)):\n",
    "                #    Ex += X[i]*X[i]\n",
    "                #El += X[i]*X[i]\n",
    "    #        stFeatures.append(Ex / El)\n",
    "    #        stFeatures.append(numpy.argmax(X))\n",
    "    #        if curFV[numOfTimeSpectralFeatures+nceps+1]>0:\n",
    "    #            print curFV[numOfTimeSpectralFeatures+nceps], curFV[numOfTimeSpectralFeatures+nceps+1]\n",
    "        return numpy.array(stFeatures)\n",
    "\n",
    "\n",
    "    \"\"\" Feature Extraction Wrappers\n",
    "     - The first two feature extraction wrappers are used to extract long-term averaged\n",
    "       audio features for a list of WAV files stored in a given category.\n",
    "       It is important to note that, one single feature is extracted per WAV file (not the whole sequence of feature vectors)\n",
    "     \"\"\"\n",
    "\n",
    "\n",
    "    def dirWavFeatureExtraction(dirName, mtWin, mtStep, stWin, stStep, computeBEAT=False):\n",
    "        \"\"\"\n",
    "        This function extracts the mid-term features of the WAVE files of a particular folder.\n",
    "        The resulting feature vector is extracted by long-term averaging the mid-term features.\n",
    "        Therefore ONE FEATURE VECTOR is extracted for each WAV file.\n",
    "        ARGUMENTS:\n",
    "            - dirName:        the path of the WAVE directory\n",
    "            - mtWin, mtStep:    mid-term window and step (in seconds)\n",
    "            - stWin, stStep:    short-term window and step (in seconds)\n",
    "        \"\"\"\n",
    "\n",
    "        allMtFeatures = numpy.array([])\n",
    "        processingTimes = []\n",
    "\n",
    "        types = ('*.wav', '*.aif',  '*.aiff')\n",
    "        wavFilesList = []\n",
    "        for files in types:\n",
    "            wavFilesList.extend(glob.glob(os.path.join(dirName, files)))\n",
    "\n",
    "        wavFilesList = sorted(wavFilesList)\n",
    "\n",
    "        for wavFile in wavFilesList:\n",
    "            [Fs, x] = audioBasicIO.readAudioFile(wavFile)            # read file\n",
    "            t1 = time.clock()\n",
    "            x = audioBasicIO.stereo2mono(x)                          # convert stereo to mono\n",
    "            if computeBEAT:                                          # mid-term feature extraction for current file\n",
    "                [MidTermFeatures, stFeatures] = mtFeatureExtraction(x, Fs, round(mtWin * Fs), round(mtStep * Fs), round(Fs * stWin), round(Fs * stStep))\n",
    "                [beat, beatConf] = beatExtraction(stFeatures, stStep)\n",
    "            else:\n",
    "                [MidTermFeatures, _] = mtFeatureExtraction(x, Fs, round(mtWin * Fs), round(mtStep * Fs), round(Fs * stWin), round(Fs * stStep))\n",
    "\n",
    "            MidTermFeatures = numpy.transpose(MidTermFeatures)\n",
    "            MidTermFeatures = MidTermFeatures.mean(axis=0)         # long term averaging of mid-term statistics\n",
    "            if computeBEAT:\n",
    "                MidTermFeatures = numpy.append(MidTermFeatures, beat)\n",
    "                MidTermFeatures = numpy.append(MidTermFeatures, beatConf)\n",
    "            if len(allMtFeatures) == 0:                              # append feature vector\n",
    "                allMtFeatures = MidTermFeatures\n",
    "            else:\n",
    "                allMtFeatures = numpy.vstack((allMtFeatures, MidTermFeatures))\n",
    "            t2 = time.clock()\n",
    "            duration = float(len(x)) / Fs\n",
    "            processingTimes.append((t2 - t1) / duration)\n",
    "        if len(processingTimes) > 0:\n",
    "            print(\"Feature extraction complexity ratio: {0:.1f} x realtime\".format((1.0 / numpy.mean(numpy.array(processingTimes)))))\n",
    "        return (allMtFeatures, wavFilesList)\n",
    "\n",
    "\n",
    "    def dirsWavFeatureExtraction(dirNames, mtWin, mtStep, stWin, stStep, computeBEAT=False):\n",
    "        '''\n",
    "        Same as dirWavFeatureExtraction, but instead of a single dir it takes a list of paths as input and returns a list of feature matrices.\n",
    "        EXAMPLE:\n",
    "        [features, classNames] =\n",
    "               a.dirsWavFeatureExtraction(['audioData/classSegmentsRec/noise','audioData/classSegmentsRec/speech',\n",
    "                                           'audioData/classSegmentsRec/brush-teeth','audioData/classSegmentsRec/shower'], 1, 1, 0.02, 0.02);\n",
    "        It can be used during the training process of a classification model ,\n",
    "        in order to get feature matrices from various audio classes (each stored in a seperate path)\n",
    "        '''\n",
    "\n",
    "        # feature extraction for each class:\n",
    "        features = []\n",
    "        classNames = []\n",
    "        fileNames = []\n",
    "        for i, d in enumerate(dirNames):\n",
    "            [f, fn] = dirWavFeatureExtraction(d, mtWin, mtStep, stWin, stStep, computeBEAT=computeBEAT)\n",
    "            if f.shape[0] > 0:       # if at least one audio file has been found in the provided folder:\n",
    "                features.append(f)\n",
    "                fileNames.append(fn)\n",
    "                if d[-1] == \"/\":\n",
    "                    classNames.append(d.split(os.sep)[-2])\n",
    "                else:\n",
    "                    classNames.append(d.split(os.sep)[-1])\n",
    "        return features, classNames, fileNames\n",
    "\n",
    "\n",
    "    def dirWavFeatureExtractionNoAveraging(dirName, mtWin, mtStep, stWin, stStep):\n",
    "        \"\"\"\n",
    "        This function extracts the mid-term features of the WAVE files of a particular folder without averaging each file.\n",
    "        ARGUMENTS:\n",
    "            - dirName:          the path of the WAVE directory\n",
    "            - mtWin, mtStep:    mid-term window and step (in seconds)\n",
    "            - stWin, stStep:    short-term window and step (in seconds)\n",
    "        RETURNS:\n",
    "            - X:                A feature matrix\n",
    "            - Y:                A matrix of file labels\n",
    "            - filenames:\n",
    "        \"\"\"\n",
    "\n",
    "        allMtFeatures = numpy.array([])\n",
    "        signalIndices = numpy.array([])\n",
    "        processingTimes = []\n",
    "\n",
    "        types = ('*.wav', '*.aif',  '*.aiff')\n",
    "        wavFilesList = []\n",
    "        for files in types:\n",
    "            wavFilesList.extend(glob.glob(os.path.join(dirName, files)))\n",
    "\n",
    "        wavFilesList = sorted(wavFilesList)\n",
    "\n",
    "        for i, wavFile in enumerate(wavFilesList):\n",
    "            [Fs, x] = audioBasicIO.readAudioFile(wavFile)            # read file\n",
    "            x = audioBasicIO.stereo2mono(x)                          # convert stereo to mono\n",
    "            [MidTermFeatures, _] = mtFeatureExtraction(x, Fs, round(mtWin * Fs), round(mtStep * Fs), round(Fs * stWin), round(Fs * stStep))  # mid-term feature\n",
    "\n",
    "            MidTermFeatures = numpy.transpose(MidTermFeatures)\n",
    "    #        MidTermFeatures = MidTermFeatures.mean(axis=0)        # long term averaging of mid-term statistics\n",
    "            if len(allMtFeatures) == 0:                # append feature vector\n",
    "                allMtFeatures = MidTermFeatures\n",
    "                signalIndices = numpy.zeros((MidTermFeatures.shape[0], ))\n",
    "            else:\n",
    "                allMtFeatures = numpy.vstack((allMtFeatures, MidTermFeatures))\n",
    "                signalIndices = numpy.append(signalIndices, i * numpy.ones((MidTermFeatures.shape[0], )))\n",
    "\n",
    "        return (allMtFeatures, signalIndices, wavFilesList)\n",
    "\n",
    "\n",
    "\n",
    "    def mtFeatureExtractionToFile(fileName, midTermSize, midTermStep, shortTermSize, shortTermStep, outPutFile,\n",
    "                                  storeStFeatures=False, storeToCSV=False, PLOT=False):\n",
    "        \"\"\"\n",
    "        This function is used as a wrapper to:\n",
    "        a) read the content of a WAV file\n",
    "        b) perform mid-term feature extraction on that signal\n",
    "        c) write the mid-term feature sequences to a numpy file\n",
    "        \"\"\"\n",
    "        [Fs, x] = audioBasicIO.readAudioFile(fileName)            # read the wav file\n",
    "        x = audioBasicIO.stereo2mono(x)                           # convert to MONO if required\n",
    "        if storeStFeatures:\n",
    "            [mtF, stF] = mtFeatureExtraction(x, Fs, round(Fs * midTermSize), round(Fs * midTermStep), round(Fs * shortTermSize), round(Fs * shortTermStep))\n",
    "        else:\n",
    "            [mtF, _] = mtFeatureExtraction(x, Fs, round(Fs*midTermSize), round(Fs * midTermStep), round(Fs * shortTermSize), round(Fs * shortTermStep))\n",
    "\n",
    "        numpy.save(outPutFile, mtF)                              # save mt features to numpy file\n",
    "        if PLOT:\n",
    "            print(\"Mid-term numpy file: \" + outPutFile + \".npy saved\")\n",
    "        if storeToCSV:\n",
    "            numpy.savetxt(outPutFile+\".csv\", mtF.T, delimiter=\",\")\n",
    "            if PLOT:\n",
    "                print(\"Mid-term CSV file: \" + outPutFile + \".csv saved\")\n",
    "\n",
    "        if storeStFeatures:\n",
    "            numpy.save(outPutFile+\"_st\", stF)                    # save st features to numpy file\n",
    "            if PLOT:\n",
    "                print(\"Short-term numpy file: \" + outPutFile + \"_st.npy saved\")\n",
    "            if storeToCSV:\n",
    "                numpy.savetxt(outPutFile+\"_st.csv\", stF.T, delimiter=\",\")    # store st features to CSV file\n",
    "                if PLOT:\n",
    "                    print(\"Short-term CSV file: \" + outPutFile + \"_st.csv saved\")\n",
    "\n",
    "\n",
    "    def mtFeatureExtractionToFileDir(dirName, midTermSize, midTermStep, shortTermSize, shortTermStep, storeStFeatures=False, storeToCSV=False, PLOT=False):\n",
    "        types = (dirName + os.sep + '*.wav', )\n",
    "        filesToProcess = []\n",
    "        for files in types:\n",
    "            filesToProcess.extend(glob.glob(files))\n",
    "        for f in filesToProcess:\n",
    "            outPath = f\n",
    "            mtFeatureExtractionToFile(f, midTermSize, midTermStep, shortTermSize, shortTermStep, outPath, storeStFeatures, storeToCSV, PLOT)\n",
    "\n",
    "\n",
    "    # #### \n",
    "\n",
    "    # In[3]:\n",
    "\n",
    "\n",
    "    code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "    emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "\n",
    "    framerate = 16000\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    import pickle\n",
    "    with open('./result.pickle', 'rb') as handle:\n",
    "        data2 = pickle.load(handle)\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    def calculate_features(frames, freq, options):\n",
    "        window_sec = 0.2\n",
    "        window_n = int(freq * window_sec)\n",
    "\n",
    "        st_f = stFeatureExtraction(frames, freq, window_n, window_n / 2)\n",
    "\n",
    "        if st_f.shape[1] > 2:\n",
    "            i0 = 1\n",
    "            i1 = st_f.shape[1] - 1\n",
    "            if i1 - i0 < 1:\n",
    "                i1 = i0 + 1\n",
    "\n",
    "            deriv_st_f = np.zeros((st_f.shape[0], i1 - i0), dtype=float)\n",
    "            for i in range(i0, i1):\n",
    "                i_left = i - 1\n",
    "                i_right = i + 1\n",
    "                deriv_st_f[:st_f.shape[0], i - i0] = st_f[:, i]\n",
    "            return deriv_st_f\n",
    "        elif st_f.shape[1] == 2:\n",
    "            deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "            deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "            return deriv_st_f\n",
    "        else:\n",
    "            deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "            deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "            return deriv_st_f\n",
    "\n",
    "\n",
    "\n",
    "    x_train_speech = []\n",
    "\n",
    "    counter = 0\n",
    "    for ses_mod in data2:\n",
    "        x_head = ses_mod['signal']\n",
    "        st_features = calculate_features(x_head, framerate, None)\n",
    "        st_features, _ = pad_sequence_into_array(st_features, maxlen=100)\n",
    "        x_train_speech.append( st_features.T )\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "    x_train_speech = np.array(x_train_speech)\n",
    "\n",
    "\n",
    "    x_train_mocap = []\n",
    "    counter = 0\n",
    "    for ses_mod in data2:\n",
    "        x_head = ses_mod['mocap_head']\n",
    "        if(x_head.shape != (200,18)):\n",
    "            x_head = np.zeros((200,18))   \n",
    "        x_head[np.isnan(x_head)]=0\n",
    "        x_hand = ses_mod['mocap_hand']\n",
    "        if(x_hand.shape != (200,6)):\n",
    "            x_hand = np.zeros((200,6))   \n",
    "        x_hand[np.isnan(x_hand)]=0\n",
    "        x_rot = ses_mod['mocap_rot']\n",
    "        if(x_rot.shape != (200,165)):\n",
    "            x_rot = np.zeros((200,165))   \n",
    "        x_rot[np.isnan(x_rot)]=0\n",
    "        x_mocap = np.concatenate((x_head, x_hand), axis=1)\n",
    "        x_mocap = np.concatenate((x_mocap, x_rot), axis=1)\n",
    "        x_train_mocap.append(x_mocap)\n",
    "\n",
    "    x_train_mocap = np.array(x_train_mocap)\n",
    "    x_train_mocap = x_train_mocap.reshape(-1,200,189,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Y=[]\n",
    "    for ses_mod in data2:\n",
    "        Y.append(ses_mod['emotion'])\n",
    "    Y = label_binarize(Y,classes=emotions_used)\n",
    "\n",
    "\n",
    "\n",
    "    class_names = ['ang', 'exc', 'neu', 'sad']\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self, embed_dim, num_heads):\n",
    "            super(CrossAttentionBlock, self).__init__()\n",
    "            self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "            self.norm = nn.LayerNorm(embed_dim)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        def forward(self, query, key_value):\n",
    "            attn_output, _ = self.cross_attn(query, key_value, key_value)\n",
    "            out = self.norm(query + self.dropout(attn_output))\n",
    "            return out\n",
    "\n",
    "    class CombinedModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CombinedModel, self).__init__()\n",
    "            self.speech_model = SpeechModel()\n",
    "            self.mocap_model = MoCapModel()\n",
    "            self.cross_attn_s2m = CrossAttentionBlock(embed_dim=256, num_heads=4)\n",
    "            self.cross_attn_m2s = CrossAttentionBlock(embed_dim=256, num_heads=4)\n",
    "            self.fc1 = nn.Linear(512, 256)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, batch_first=True)\n",
    "            self.fusion_transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "            self.fc2 = nn.Linear(256, 4)\n",
    "\n",
    "        def forward(self, x_speech, x_mocap):\n",
    "            out_speech = self.speech_model(x_speech)\n",
    "            out_mocap = self.mocap_model(x_mocap)\n",
    "            out_speech = out_speech.unsqueeze(1)\n",
    "            out_mocap = out_mocap.unsqueeze(1)\n",
    "            speech_attended = self.cross_attn_s2m(out_speech, out_mocap)\n",
    "            mocap_attended = self.cross_attn_m2s(out_mocap, out_speech)\n",
    "            combined = torch.cat((speech_attended, mocap_attended), dim=-1).squeeze(1)\n",
    "            x = F.relu(self.fc1(combined)).unsqueeze(1)\n",
    "            x = self.fusion_transformer(x).squeeze(1)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    class SpeechModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SpeechModel, self).__init__()\n",
    "            config = T5Config(\n",
    "                d_model=128,\n",
    "                d_kv=32,\n",
    "                d_ff=256,\n",
    "                num_layers=4,\n",
    "                num_heads=4,\n",
    "                relative_attention_num_buckets=8,\n",
    "                feed_forward_proj=\"relu\",\n",
    "                dropout_rate=0.1,\n",
    "            )\n",
    "            self.encoder = T5EncoderModel(config)\n",
    "            self.input_proj = nn.Linear(34, 128)\n",
    "            self.fc = nn.Linear(128, 256)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.input_proj(x)\n",
    "            attention_mask = torch.ones(x.size()[:2], dtype=torch.long, device=x.device)\n",
    "            encoder_outputs = self.encoder(inputs_embeds=x, attention_mask=attention_mask)\n",
    "            pooled = encoder_outputs.last_hidden_state.mean(dim=1)\n",
    "            x = self.fc(pooled)\n",
    "            return x\n",
    "\n",
    "    class MoCapModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MoCapModel, self).__init__()\n",
    "            self.resnet = models.resnet18(pretrained=False)\n",
    "            self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 256)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.resnet(x)\n",
    "    model = CombinedModel()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dic =  torch.load('combined_model.pth', map_location=device)\n",
    "    model.load_state_dict(dic)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    result_lines = [f\"Video name: {tar}\\n\"]\n",
    "\n",
    "    for i in range(len(x_train_speech)):\n",
    "        speech_input = torch.tensor(x_train_speech[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        mocap_input = torch.tensor(x_train_mocap[i], dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        true_label = np.argmax(Y[i]) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(speech_input, mocap_input)  # [1, 4]\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_index = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_index].item()\n",
    "\n",
    "        class_names = ['ang', 'exc', 'neu', 'sad']\n",
    "        print(ts[i])\n",
    "        print(f\"Predicted: {class_names[predicted_index]} ({predicted_index}), Confidence: {confidence:.2f}\")\n",
    "        print(f\"True label: {class_names[true_label]} ({true_label})\")\n",
    "\n",
    "\n",
    "\n",
    "        result_lines.append(f\"Time: {\"--\".join([str(i) for i in ts[i]])}\")\n",
    "        result_lines.append(f\"Predicted: {class_names[predicted_index]}, Confidence: {confidence:.2f}\")\n",
    "        result_lines.append(f\"True label: {class_names[true_label]} \\n\")\n",
    "        emotion_at_time.append(\n",
    "        {\"start\": ts[i][0], \"end\": ts[i][1], \"emotion\": class_names[predicted_index], \"conf\": confidence})\n",
    "    output_text.delete(\"1.0\", tk.END)\n",
    "    output_text.insert(tk.END, \"\\n\".join(result_lines))\n",
    "    \n",
    "        # with open(f\"{tar}_results.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        #     file.write(\"\\n\".join(result_lines))\n",
    "\n",
    "\n",
    "current_tar = \"\"\n",
    "total_seconds = 0\n",
    "fps = 25\n",
    "user_dragging = False\n",
    "\n",
    "def run_recognition():\n",
    "    if current_tar:\n",
    "        f(current_tar)\n",
    "\n",
    "def select_file():\n",
    "    global cap, is_paused, current_tar, total_seconds, fps\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov\")])\n",
    "    if file_path:\n",
    "        current_tar = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        if cap:\n",
    "            cap.release()\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        total_seconds = int(total_frames / fps)\n",
    "        frame_slider.config(to=total_seconds)\n",
    "        time_label.config(text=\"0 / \" + str(total_seconds) + \" sec\")\n",
    "        is_paused = False\n",
    "        play_video()\n",
    "\n",
    "def toggle_pause():\n",
    "    global is_paused\n",
    "    is_paused = not is_paused\n",
    "    if not is_paused:\n",
    "        play_video()\n",
    "\n",
    "def on_slider_press(event):\n",
    "    global user_dragging\n",
    "    user_dragging = True\n",
    "\n",
    "def on_slider_release(event):\n",
    "    global user_dragging\n",
    "    second = frame_slider.get()\n",
    "    if cap:\n",
    "        frame_idx = int(second * fps)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    user_dragging = False\n",
    "    if not is_paused:\n",
    "        play_video()\n",
    "\n",
    "def play_video():\n",
    "    global cap, is_paused, fps\n",
    "    if cap and cap.isOpened() and not is_paused:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "            second = current_frame / fps\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (640, 360))\n",
    "            img = Image.fromarray(frame)\n",
    "            imgtk = ImageTk.PhotoImage(image=img)\n",
    "            video_label.imgtk = imgtk\n",
    "            video_label.config(image=imgtk)\n",
    "            if not user_dragging:\n",
    "                frame_slider.set(int(second))\n",
    "            time_label.config(text=f\"{int(second)} / {total_seconds} sec\")\n",
    "\n",
    "            # 更新当前情感显示\n",
    "            emotion = \"none\"\n",
    "            for item in emotion_at_time:\n",
    "                if item[\"start\"] <= second <= item[\"end\"]:\n",
    "                    emotion = f\"{item['emotion']} ({item['conf']:.2f})\"\n",
    "                    break\n",
    "            current_emotion_label.config(text=f\"emotion: {emotion}\")\n",
    "\n",
    "            video_label.after(33, play_video)\n",
    "        else:\n",
    "            cap.release()\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Player\")\n",
    "root.geometry(\"1200x700\")\n",
    "root.configure(bg=\"#f2f2f2\")\n",
    "\n",
    "main_frame = tk.Frame(root, bg=\"#f2f2f2\")\n",
    "main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=20)\n",
    "\n",
    "video_frame = tk.Frame(main_frame, bg=\"#f2f2f2\")\n",
    "video_frame.grid(row=0, column=0, padx=10, pady=10)\n",
    "\n",
    "video_label = Label(video_frame, bg=\"#000\")\n",
    "video_label.pack()\n",
    "\n",
    "frame_slider = Scale(video_frame, from_=0, to=100, orient=HORIZONTAL, length=640, showvalue=0)\n",
    "frame_slider.pack(pady=5)\n",
    "frame_slider.bind(\"<ButtonPress-1>\", on_slider_press)\n",
    "frame_slider.bind(\"<ButtonRelease-1>\", on_slider_release)\n",
    "\n",
    "time_label = Label(video_frame, text=\"0 / 0 sec\", font=(\"Arial\", 12), bg=\"#f2f2f2\")\n",
    "time_label.pack()\n",
    "\n",
    "result_frame = tk.Frame(main_frame, bg=\"#f2f2f2\")\n",
    "result_frame.grid(row=0, column=1, padx=10, pady=10, sticky=\"n\")\n",
    "\n",
    "Label(result_frame, text=\"result\", font=(\"Arial\", 16), bg=\"#f2f2f2\").pack(pady=5)\n",
    "output_text = Text(result_frame, height=25, width=50, font=(\"Courier\", 11))\n",
    "output_text.pack()\n",
    "\n",
    "control_frame = tk.Frame(root, bg=\"#f2f2f2\")\n",
    "control_frame.pack(side=tk.BOTTOM, pady=15)\n",
    "\n",
    "btn_style = {\"font\": (\"Arial\", 12), \"bg\": \"#4CAF50\", \"fg\": \"white\", \"padx\": 15, \"pady\": 5}\n",
    "\n",
    "Button(control_frame, text=\"choose video\", command=select_file, **btn_style).pack(side=tk.LEFT, padx=10)\n",
    "Button(control_frame, text=\"play / pause\", command=toggle_pause, **btn_style).pack(side=tk.LEFT, padx=10)\n",
    "Button(control_frame, text=\"run detection\", command=run_recognition, **btn_style).pack(side=tk.LEFT, padx=10)\n",
    "current_emotion_label = Label(video_frame, text=\"emotion: none\", font=(\"Arial\", 14), fg=\"blue\", bg=\"#f2f2f2\")\n",
    "current_emotion_label.pack(pady=5)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69eb42-1473-4cd6-8c33-d7ad3a03e36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
